{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Version 2.3.1</p>"},{"location":"#parameters-analyzer","title":"Parameters Analyzer","text":"<p>Hello and welcome to Parameters Analyzer docs! </p> <p>Parameters Analyzer by Dataiku is designed to help you as production, quality or process engineer. With this app, you can  identify key parameters of your production setup and how they affect output quality with a data-driven approach.</p> <p>For a quick overview, watch the demo below. Use the chapters and navigation for a deep dive, or use search to quickly find an answer. </p> <p>Questions? Compliments? Complaints? Reach out to us at team [a] dataiku.com ##todo</p> <p>note I don't know why sound is out of sync and this is just an example</p> <p></p>"},{"location":"#index","title":"Index","text":"<ul> <li> <p> Usage</p> <p>Using and living with this app.</p> <p> Walkthrough  Data preparation  Flow  Case study </p> </li> <li> <p> Data</p> <p>Data, preparation and computation.</p> <p> Data Model  Data Preparation  Processing Mode </p> </li> <li> <p> Advanced</p> <p>Data, preparation and computation.</p> <p> Data Model  Scenario  Processing Mode  Requirements </p> </li> </ul>"},{"location":"case-study/","title":"Case Study","text":"<p>This case study walks you through Parameters Analyzer using synthetic data from a plastic injection process.</p>"},{"location":"case-study/#user-story","title":"User Story","text":"<p>As a process engineer, you design processes and equipment, monitor through testing, and provide strategic oversight. You want to use data analytics to identify trends and improvement areas to make informed decisions about process efficiency.</p> <p>As a quality engineer, you inspect and evaluate products for durability, function, and quality. You need a simple application to find insights on recent production results, understand the impact of parameter changes, and better grasp overall process behavior.</p>"},{"location":"case-study/#data","title":"Data","text":"<p>The data in this application comes from synthetic datasets modeled on a plastic injection process. These datasets simulate real production scenarios, capturing key parameters, outputs, and variability.</p> <p>For detailed information about the data structure and field definitions, see the Data Model page.</p>"},{"location":"case-study/#insights","title":"Insights","text":"<p>Parameters Analyzer helps you uncover the most impactful parameters on a specified target and identify optimal tuning ranges based on historical data.</p> <p>Using synthetic plastic injection process data, the application shows you how to:</p> <ol> <li>Identify key factors driving your target outcome</li> <li>Extract actionable insights from past tunings and configurations</li> <li>Find improvements to enhance production quality and efficiency</li> </ol> <p>This approach ensures you can quickly apply insights to your real production data.</p>"},{"location":"data-model/","title":"Data Model","text":"<p>Parameters Analyzer works with two data model options:</p> <ul> <li>Unified dataset containing event IDs, timestamps, parameters and quality classification (e.g., <code>event,timestamp,quality,parameter</code> header row)</li> <li>Separated datasets containing events with at least <code>event_id</code> and <code>timestamp</code>, plus a target dataset with quality labels. The application matches event IDs and merges all columns.</li> </ul>"},{"location":"data-model/#unified-dataset","title":"Unified Dataset","text":"<p>The solution works with data aggregated to process events, such as product or batch. Only one dataset is expected, called the product database.</p> <p>Product Database dataset</p> <p>This dataset includes aggregated process information with outcome measurements. The schema is flexible but requires: - A product identifier - At least one manufacturing process date - At least one process measure or parameter</p> <p>General schema:</p> <ul> <li><code>product_identifier</code> (string): Unique product ID</li> <li><code>identifier_1</code> ... <code>identifier_n</code> (string): Various product/process identifiers</li> <li><code>process_parameter_1</code> ... <code>process_parameter_n</code> (varies): Process parameters</li> <li><code>process_outcome_1</code> ... <code>process_outcome_2</code> (decimal, integer, boolean, etc.): Process outcomes</li> <li><code>date_1</code> ... <code>date_n</code> (Date): Manufacturing process dates</li> </ul> <p>Identifiers pinpoint specific entities (product, process, machine). Process parameters are adjustable values affecting product quality. Process measures are event-aggregated properties recorded from the product/process. Dates are timestamps related to manufacturing.</p> <p>Example dataset: agregated_sample_data</p>"},{"location":"data-model/#separated-datasets","title":"Separated Datasets","text":"<p>This solution requires two input datasets:</p> <ol> <li>Process parameters dataset: Contains parameter instances on specific events in wide format:</li> </ol> <p> </p> <ul> <li><code>event_id</code> (string): Event's unique identifier</li> <li><code>equipment_id</code> (string): (Optional) Equipment's unique identifier</li> <li><code>spec_id</code> (string): Parameter set unique identifier</li> <li><code>process_parameter_1</code> ... <code>process_parameter_n</code> (float): Parameters values</li> <li><code>date</code> (date): Date of parameter instantiation</li> </ul> <p>Note: The schema accepts multiple parameter sets and process parameters.</p> <ol> <li>Events Information Dataset: Contains event information:</li> </ol> <p></p> <ul> <li><code>event_id</code> (string): Event's unique identifier</li> <li><code>equipment_id</code> (string): (Optional) Equipment's unique ID</li> <li><code>start_date</code> (date): (Optional) Start date of the event</li> <li><code>end_date</code> (date): (Optional) End date of the event</li> <li><code>outcome</code> (string / date / boolean / float): Process outcome measurement</li> </ul> <p>Note: The schema accepts multiple outcomes.</p>"},{"location":"data-prep/","title":"Data Preparation","text":"<p>Parameters Analyzer requires your data in one of two formats:</p> <ul> <li>Unified dataset with event IDs, timestamps, parameters and quality classification (e.g. a <code>event,timestamp,quality,parameter</code> header row)</li> <li>Two separated datasets with parameters, event IDs and timestamps, plus a target dataset of parameters. The application matches event IDs and merges all columns (using a <code>left join</code> recipe step). </li> </ul> <p>See data model for complete details.</p> <p>Note: Data preparation is critical for Parameters Analyzer. While the application can unify separate datasets, we recommend preparing data into a single dataset in a separate project. This gives you full control over preparation steps and avoids implicit assumptions that might affect interpretation or cause issues in joining your data.</p>"},{"location":"data-prep/#data-preparation-recommendations","title":"Data Preparation Recommendations","text":"<p>Follow these guidelines to ensure accurate results aligned with the required data model:</p> <ol> <li>Ensure correct data types</li> <li>Parse dates</li> <li>Use valid column names</li> <li>Remove columns with only empty values</li> <li>Handle target column empty values</li> <li>Optional: remove columns with many empty values</li> <li>Optional: remove duplicated rows</li> <li>Optional: remove highly correlated columns</li> <li>Optional: optimize by splitting datasets and/or encoding categoricals</li> </ol> <p>New to data preparation in Dataiku? Follow our Data Preparation Quick Start training.</p>"},{"location":"data-prep/#1-ensure-correct-data-types","title":"1. Ensure correct data types","text":"<ul> <li>Verify numerical columns are stored as numerical types for proper processing</li> <li>Update column types directly in Dataiku's dataset schema if needed</li> </ul>"},{"location":"data-prep/#2-parse-dates","title":"2. Parse dates","text":"<ul> <li>Make all date columns correctly parsed for compatibility</li> <li>Use a prepare recipe as recommended in Dataiku documentation on parsing dates</li> </ul>"},{"location":"data-prep/#3-use-valid-column-names","title":"3. Use valid column names","text":"<ul> <li>Rename columns using a data preparation recipe</li> <li>Use only common characters and enable column naming checks in the data preparation</li> </ul>"},{"location":"data-prep/#4-remove-columns-with-only-empty-values","title":"4. Remove columns with only empty values","text":"<ul> <li>Empty columns add no value and increase computation time</li> <li>Use Data Quality Rules to check for empty columns (Column values are empty)</li> <li>Remove empty columns using the Dataiku preparation recipe</li> </ul>"},{"location":"data-prep/#5-target-column-with-some-empty-values","title":"5. Target column with some empty values","text":"<ul> <li>Filter and remove empty values in your target column before analysis</li> <li>Use Data Quality Rules to check for empty values</li> <li>Remove empty values using Dataiku preparation or filter recipe</li> </ul>"},{"location":"data-prep/#6-optional-remove-columns-with-many-empty-values","title":"6. Optional: remove columns with many empty values","text":"<ul> <li>Columns with &gt;50% empty cells may increase computation time without adding value</li> <li>Use Data Quality Rules to calculate empty cell percentage</li> <li>Consider removing these columns unless critical to your analysis</li> </ul>"},{"location":"data-prep/#7-optional-remove-duplicated-rows","title":"7. Optional: remove duplicated rows","text":"<ul> <li>Duplicated rows increase computation time and may affect results</li> <li>Use the Distinct Recipe to keep only unique rows</li> </ul>"},{"location":"data-prep/#8-optional-remove-highly-correlated-columns","title":"8. Optional: remove highly correlated columns","text":"<ul> <li>Remove highly correlated columns for better performance and reduced redundancy</li> <li>Generate a correlation matrix to identify redundant columns:</li> <li>Click on Statistics tab</li> <li>Select Multivariate analysis then Correlation matrix</li> <li>Select columns to analyze</li> <li>Generate the matrix to identify redundant columns</li> </ul>"},{"location":"data-prep/#9-optional-optimize-processing","title":"9. Optional: optimize processing","text":"<ul> <li>Split datasets with many columns to reduce processing impact and improve interpretation</li> <li>Consider encoding categorical columns (stored as <code>string</code>) into numerical types for faster processing</li> </ul>"},{"location":"flow/","title":"Parameters Analyzer Flow","text":"<p>The \"flow\" of Parameters Analyzer contains the dataset and meta storage needed for the application to run.</p> <p>Note: Running embedded scenarios might affect your flow. If keeping your flow or data unmodified is important, we strongly recommend using a secondary project.</p>"},{"location":"flow/#new-instances","title":"New Instances","text":"<p>When creating a Parameters Analyzer instance, the initial flow is straightforward: it contains only a \"saved studies\" dataset to store your conducted studies.</p> <p></p>"},{"location":"flow/#adding-data","title":"Adding Data","text":"<p>When you start the application with data, the dataset is added to the flow in one of two ways:</p> <ul> <li>Native project datasets appear as regularly connected datasets</li> <li>Foreign datasets (from other projects) appear as symbolic links (black icon) to avoid unnecessary duplication. </li> </ul> <p></p> <p>Note: Preparing your data in a separate project is strongly recommended to make sure your flows are not altered by the automated scenarios in Parameters Analyzer. </p>"},{"location":"flow/#common-flows","title":"Common Flows","text":"<p>A common flow contains the meta storage (studies) and an automatically created flow when used with separate dataset will look something like this:</p> <p></p>"},{"location":"legal/","title":"Legal","text":""},{"location":"legal/#legal-disclaimer-on-solution-data","title":"Legal Disclaimer on Solution Data","text":"<p>This Solution is meant to be used as a guide for the development of your analysis in Dataiku. The results of the model should not be used as actionable insights and the data provided with the Solution may not be representative of actual data in a real-life project. Dataiku makes no representations or warranties regarding the performance, availability, or results that may be obtained from using our Business Solution, including with your own data. The use of these Solutions is at your own risk.</p>"},{"location":"methodology/","title":"Methods and Computations","text":"<p>This section details specific assumptions and methods used by Parameters Analyzer.</p>"},{"location":"methodology/#target-selection","title":"Target Selection","text":"<p>Once you select the target variable and its OK range or values, it's binarized into 2 classes: OK and NOK (\"Not OK\").</p>"},{"location":"methodology/#data-assumptions-and-definitions","title":"Data Assumptions and Definitions","text":"<p>Outlier filtering for numerical variables: Values below the 1.5th percentile and above the 98.5th percentile are excluded to remove extreme outliers that distort analysis. This ensures more representative results, clearer graphs, and actionable conclusions. You can disable outlier filtering by setting <code>outliers_filtering</code> to <code>False</code>.</p> <p>Binning of values: This divides data for each variable into \"bins\" or intervals. For numerical variables, numpy.histogram determines bin edges based on value distribution. For categorical variables, each unique category becomes a bin. Only the 30 most populated modalities are displayed.</p> <p>By default, 10 bins are computed. Change the tab setting to increase bin resolution up to 30 bins.</p> <p>Default rates of variables: For each variable, the average NOK rate is calculated as the proportion of values outside the target variable's good range.</p> <p>Default rates of bins: For each bin, the defect rate is calculated and compared to the variable's average NOK rate.</p> <p>Impact of variables: Impact is calculated as the product of the absolute difference between the bin's default rate and the variable's average default rate, multiplied by the proportion of data points in the bin. Bins with default rates much higher or lower than average and containing many data points have high impact.</p> <p>Ranking of variables: For each variable, the absolute impact of all bins is summed. Variables are ranked by importance, with the highest total impact first. This importance measure is normalized between 0 and 100 and determines the top 30 most important variables to display.</p> <p>Display of results: Each graph shows the variable's average default rate. Bubbles are positioned based on their default rate, with size corresponding to points in the bin. Color is red if the bin's default rate exceeds the variable's default rate, green otherwise.</p>"},{"location":"performance-limitations/","title":"Performance and Limitations","text":"<p>Parameters Analyzer is designed to operate under specific conditions. Understanding these principles helps you troubleshoot any performance issues.</p>"},{"location":"performance-limitations/#performance-recommendations","title":"Performance Recommendations","text":"<p>Performance depends on three main factors: data preparation, dataset characteristics, and storage/processing choice:</p> <ul> <li>Follow the data preparation recommendations</li> <li>Downsize your data where possible for your analysis (smaller is faster)</li> <li>Split datasets with more than 50 columns (slimmer is faster)</li> <li>Use pushdown mode for significantly better performance on larger datasets</li> </ul>"},{"location":"performance-limitations/#data-storage-requirements","title":"Data &amp; Storage Requirements","text":"<p>The application requires at least one date parsed to ISO 8601 format (<code>yyyy-MM-ddTHH:mm:ss.SSSZ</code>) as done in Dataiku.</p> <p>Data types are taken from the dataset schema, not inferred. Ensure:</p> <ul> <li>Numerical variables are stored as numerical types</li> <li>Categorical variables are stored as String</li> </ul>"},{"location":"performance-limitations/#incorrect-storage-example","title":"Incorrect Storage Example","text":"<p>Numerical variable stored as <code>String</code>: The  icon indicates a categorical variable. The chart isn't ordered properly and shows a modality list instead of a slider.</p> <p></p>"},{"location":"performance-limitations/#correct-storage-example","title":"Correct Storage Example","text":"<p>Numerical variable stored correctly: The  icon indicates a numerical variable with proper display.</p> <p></p>"},{"location":"performance-limitations/#performance-analysis","title":"Performance Analysis","text":"<p>We've tested different data sizes and storage types with Parameters Analyzer. As of Release 2.3, the Processing Mode option (<code>in-memory</code> or <code>pushdown</code>) affects performance positively (with larger dataset significantly more performant).</p> <p>These benchmarks provide expectations based on data size and composition. For pushdown mode, performance relates directly to your database configuration.</p> <p>Storage and processing environments tested:</p> <ul> <li>Local DKU PG: AWS <code>m5.xlarge</code> running PostgreSQL</li> <li>Amazon RDS: Amazon RDS for PostgreSQL</li> <li>MacBook M3: Apple MacBook Pro with M3 chip</li> </ul> <p>Dataset naming format: <code>&lt;size&gt;_&lt;columns&gt;_&lt;rows&gt;</code> - Size: XS (Extra Small), S (Small), M (Medium), L (Large) - Columns: Number of columns - Rows: Number of rows</p> <p>Example: <code>M_62_860k</code> has 62 columns and 860,000 rows</p>"},{"location":"performance-limitations/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>You can expect analysis of less than a million rows with tens of columns to complete in tens of seconds. Adding columns significantly increases processing time, as dataset size grows by a  <code>rows * N</code> per column.</p>"},{"location":"performance-limitations/#datasets-with-varied-rows-62-columns","title":"Datasets with Varied Rows (62 Columns)","text":"Dataset Columns Rows Local DKU PG (s) Amazon RDS (s) MacBook M3 (s) XS 62 10k 62 10,000 1.1 2.5 3.9 S 62 86k 62 86,000 4.8 10 5.8 M 62 860k 62 860,000 50 38 41.3 L 62 2M 62 2,000,000 154 168 120"},{"location":"performance-limitations/#datasets-with-varied-columns-860k-rows","title":"Datasets with Varied Columns (860k Rows)","text":"Dataset Columns Rows Local DKU PG (s) Amazon RDS (s) MacBook M3 (s) XS 62 860k 62 860,000 50 38 39 S 100 860k 100 860,000 120 91 124 M 200 860k 200 860,000 210 155 948 L 500 860k 500 860,000 2074 897 N/A"},{"location":"performance-limitations/#notes","title":"Notes:","text":"<ol> <li>All times in whole seconds</li> <li>\"N/A\" indicates incomplete processes due to resource constraints</li> <li>Performance varies significantly by dataset size and infrastructure</li> <li>Increasing columns impacts execution time more than increasing rows</li> <li>Numerical data typically scales better than categorical data</li> </ol>"},{"location":"processing-mode/","title":"Processing Mode","text":"<p>Version 2.3 of Parameters Analyzer introduced two processing modes: <code>pushdown</code> or <code>memory</code>. While changing this setting is an admin task (via global_variables), understanding the difference is important for all users.</p>"},{"location":"processing-mode/#processing-options","title":"Processing Options","text":"<p>Parameters Analyzer offers two processing approaches to optimize your experience based on data size:</p> <ul> <li>Pushdown mode leverages your underlying database engine for handling large datasets</li> <li>Memory mode provides a faster experience for smaller datasets by loading everything into application memory</li> </ul>"},{"location":"processing-mode/#when-to-use-pushdown-mode","title":"When to Use Pushdown Mode","text":"<ul> <li>Working with very large datasets (1GB+) or wide datasets (many columns)</li> <li>For routine analysis with established parameters</li> </ul>"},{"location":"processing-mode/#when-to-use-memory-mode","title":"When to Use Memory Mode","text":"<ul> <li>Working with smaller datasets (&lt;1GB)</li> <li>When exploring data and preparing for larger analyses</li> </ul>"},{"location":"processing-mode/#advanced-deeper-dive","title":"Advanced: Deeper Dive","text":"<p>Unlike standard Dataiku flows (pipelines), Parameters Analyzer provides an interactive experience where all data must be available while the application runs.</p> <p>Parameters Analyzer follows this pattern:</p> <ul> <li>Data loads from a DSS dataset (external or DSS native storage)</li> <li>Data is held in an efficient storage abstraction</li> <li>All interactions use standardized SQL queries</li> <li>These queries run on either:</li> <li>Your database engine (in <code>pushdown</code> mode)</li> <li>An in-memory analytical database (in <code>memory</code> mode)</li> </ul> <p>This setup adapts to your needs (either quick response or large dataset capability) while providing an interactive experience.    </p>"},{"location":"processing-mode/#schematic-overview","title":"Schematic overview","text":""},{"location":"release-notes/","title":"Release notes","text":""},{"location":"release-notes/#version-232-update-release-2025-05","title":"Version 2.3.2 - Update Release - 2025-05","text":"<ul> <li>Added automatic system turn off when idle</li> <li>Added Snowflake compatibility for computation pushdown</li> </ul>"},{"location":"release-notes/#version-231-bug-fixes-2025-04","title":"Version 2.3.1 - Bug fixes - 2025-04","text":"<ul> <li>Fixed bug in databricks connection</li> <li>Fixed bug when interacting with boolean columns</li> </ul>"},{"location":"release-notes/#version-230-update-release-2025-04","title":"Version 2.3.0 - Update Release - 2025-04","text":""},{"location":"release-notes/#major-improvements","title":"Major improvements","text":"<ul> <li>Added capability to delegate all computations to the database containing the product data. </li> <li>Improved performance for small datasets using the \"in-memory\" option.</li> <li>General improvements to architecture, application quality and maintability.</li> </ul> <p>Handling bigger (massive) datasets is compatible with Databricks and PostgreSQL. Performance is directly related to available computation power: the bigger the underlying database cluster, the faster Parameters Analyzer will run on big datasets. </p>"},{"location":"release-notes/#minor-improvements","title":"Minor improvements","text":"<ul> <li>Removed the 50k limit on the number of points for the target chart.</li> <li>Eliminated initial data duplication.</li> <li>Enhanced application launch performance.</li> <li>Fixed a minor bug in outliers computation.</li> </ul>"},{"location":"release-notes/#version-210-stability-release-2024-12","title":"Version 2.1.0 - Stability Release - 2024-12","text":"<p>The most important changes for the December 2024 release (version 2.1.0):</p> <ul> <li>Important logical changes:<ul> <li>Adjusted the NOK rate computation to avoid issues with empty fields, see Methodology.</li> <li>In some cases score computation could be larger than 100%, fixed. </li> </ul> </li> <li>UX improvements:<ul> <li>Introducing a helpful step to check column naming when preparing data (see Requirements &amp; data model).</li> <li>Selecting the date column in the Dataiku app before launching now only includes <code>date</code> columns.</li> </ul> </li> <li>In these docs, added additional clarity on the assumptions Parameters Analyzer makes about your data and tested limitations. </li> <li>Added Filesystem as supported storage type. Note that filesystem can be less performant than SQL processing on larger datasets.   </li> <li>Saving studies and working on a copy of study is now fixed.</li> <li>Overall bugfixes and minor improvements to front- and backend.</li> <li>(For technical users) general improvements to logging</li> </ul>"},{"location":"release-notes/#version-203-bug-fixes-2024-09","title":"Version 2.0.3 - Bug fixes - 2024-09","text":"<ul> <li>Fixed bugs in \"local\" nok rate computing </li> <li>Filter message fix for categorical variables</li> </ul>"},{"location":"release-notes/#version-202-update-release-2024-09","title":"Version 2.0.2 - Update Release - 2024-09","text":"<ul> <li>Fixed bugs in backend</li> <li>Fixed bugs in frontend</li> <li>Enhanced scenario</li> <li>Enhanced defect_binary column handling in backend</li> <li>Several UX improvements with filters</li> </ul>"},{"location":"release-notes/#version-201-bug-fixes-2024-08","title":"Version 2.0.1 - Bug fixes - 2024-08","text":"<ul> <li>Fixed bugs in frontend</li> <li>Fixed bugs in scenario</li> </ul>"},{"location":"release-notes/#version-200-update-release-2024-08","title":"Version 2.0.0 - Update Release - 2024-08","text":"<ul> <li>Complete redesign of the Dataiku Application</li> <li>Added filtering in the web Application</li> <li>Make outliers filtering deactivable</li> <li>Charts unified between Standard and Expert mode</li> </ul>"},{"location":"release-notes/#version-123-bug-fixes-2024-07","title":"Version 1.2.3 - Bug fixes - 2024-07","text":"<ul> <li>Adjusted Initial build scenario to avoid schema update bug of Dataiku</li> </ul>"},{"location":"release-notes/#version-122-update-release-2024-07","title":"Version 1.2.2 - Update Release - 2024-07","text":"<ul> <li>Added Snowflake compatibility</li> <li>Added MS SQL compatibility</li> <li>Adjusted schema inference logic</li> <li>Adjusted [FileUpload] Input Reconfiguration scenario to avoid schema update bug of Dataiku</li> <li>Adjusted casting logic in SQL connections</li> <li>Fixed bug in statistics when using Dataiku &lt; 12.6</li> <li>Renamed 'Correlation' to 'Impact' in charts</li> <li>Fixed bug in handling boolean variables</li> </ul>"},{"location":"release-notes/#version-121-update-release-2024-06","title":"Version 1.2.1 - Update Release - 2024-06","text":"<ul> <li>Rolled back schema inference</li> <li>Fixed bug in target caching logic</li> <li>Fixed bug in scoring to avoid rank &lt; 1</li> <li>Improved target handling robustness</li> <li>Fixed bug in handling booleans (in variables and target)</li> <li>Minor bug fixes</li> </ul>"},{"location":"release-notes/#version-120-update-release-2024-05","title":"Version 1.2.0 - Update Release - 2024-05","text":"<ul> <li>Improved performance in PostgreSQL</li> <li>Enhanced robustness, particularly in handling variables containing integers</li> <li>Better management of studies loading</li> </ul>"},{"location":"release-notes/#version-112-bug-fixes-2024-04","title":"Version 1.1.2 - Bug fixes - 2024-04","text":"<ul> <li>Fixed bug in Snowflake connection</li> <li>Fixed bug in 'mode' statistics display</li> </ul>"},{"location":"release-notes/#version-111-update-release-2024-03","title":"Version 1.1.1 - Update Release - 2024-03","text":"<ul> <li>Fixed bug for Dataiku cloud</li> <li>Adjust categorical variables detection</li> <li>Minor bug fixes</li> </ul>"},{"location":"release-notes/#version-110-update-release-2024-03","title":"Version 1.1.0 - Update Release - 2024-03","text":"<ul> <li>All columns displayed in dataiku app, date parsed in the backend</li> <li>Dataiku app message to explain how to select a table</li> <li>New statistics when target is selected</li> <li>Fix bug statistics not updated in study definition</li> <li>Improve user experience when defining study (remove all variables, confirmation popin when saving,..)</li> <li>Various front end improvements</li> <li>Update web app infrastructure</li> </ul>"},{"location":"release-notes/#version-102-bug-fixes-2024-03","title":"Version 1.0.2 - Bug fixes - 2024-03","text":"<ul> <li>Fixed bug on scenarios error</li> <li>Add exceptions in initial build scenario if empty dataset or selected date doesn't exist</li> <li>Minor bug fixes in web app</li> <li>Auto date parsing fix</li> <li>Renaming solution</li> <li>Fixed bug on \"load from database\" option</li> </ul>"},{"location":"release-notes/#version-100-initial-release-2023-12","title":"Version 1.0.0 - Initial Release - 2023-12","text":"<ul> <li>Load a csv or connect to a SQL-like database with process and outcome data</li> <li>Run analysis through the web app</li> <li>Set and save rules</li> <li>See the saved rules</li> <li>Work on a copy of a saved rule</li> </ul>"},{"location":"scenario/","title":"Scenario","text":"<p>Scenarios in Parameters Analyzer help in preparing your data and environment. </p> <p>They handle:</p> <ul> <li>Preparing your data (creating datasets and pointing the application to your data)</li> <li>Checking your data shape for database compatibility</li> <li>(Re)building code environments (for admins)</li> </ul> <p>These scenarios are packaged with the application code and cannot be customized. </p>"},{"location":"scenario/#checking-columns","title":"Checking Columns","text":"<p>The data preparation step checks column naming against recommendations. These are based on the overlapping character set supported by all database engines Parameters Analyzer can use in pushdown mode (only letters, numbers, spaces, hyphens, and underscores). </p> <p>Enable column checking by changing <code>\"data_check\": false</code> to <code>true</code> in variables.</p> <p>When enabled, the data preparation step checks your data shape and provides helpful messages for non-conforming columns:</p> <pre><code>Data validation failed:\n- Column 'Mixture \u00b0C' contains invalid characters: ['\u00b0']\n- Column 'Recipe, Version' contains invalid characters: [','] \n- Column 'Batch#' contains invalid characters: ['#']\n</code></pre> <p>Important: Data checks are designed not to break existing deployments and must be enabled explicitly (see variables). If set to <code>true</code>, the data preparation scenario will not complete if column names don't conform.</p>"},{"location":"scenario/#code-environment","title":"Code Environment","text":"<p>As an admin, you can run the code_environment scenario step to automatically configure the code environment for this application.</p>"},{"location":"technical-requirements/","title":"Technical Requirements","text":""},{"location":"technical-requirements/#instance-requirements","title":"Instance Requirements","text":"<p>Parameters Analyzer requires Dataiku V13.3+</p>"},{"location":"technical-requirements/#code-environment","title":"Code Environment","text":"<p>The application uses the code environment solution_parameters-analyzer with Python version <code>3.9</code>.</p> <p>Required packages:</p> <pre><code>arrow==1.3.0\nboto3==1.37.13\nduckdb==1.2.1\nFlask-Caching==2.3.1\nhumanize==4.12.2\npandas==2.2.3\npolars==1.25.2\npsutil==7.0.0\npyarrow==19.0.1\npydantic==2.10.6\npython-dotenv==1.0.1\nsqlglot==26.10.1\n</code></pre> <p>As admin, you can run the code environment scenario step to create or update this environment.</p>"},{"location":"technical-requirements/#processing-database-and-connections","title":"Processing, Database and Connections","text":"<p>Version 2.3 introduced two processing modes: in-memory and pushdown (set through variables).</p> <ul> <li>In-memory option is compatible with any connection (for smaller datasets)</li> <li>Pushdown option is tested with PostgreSQL, Snowflake and Databricks (support for other database engines is planned)</li> </ul> <p>With in-memory processing, all computation happens directly in the application instance. This is significantly faster for smaller datasets and with limited users.</p> <p>For large datasets, Databricks and Snowflake are much more efficient as they're designed for large analytical datasets.</p>"},{"location":"variables/","title":"Variables","text":"<p>Parameters Analyzer supports several settings you can control via the settings pane in DSS.</p> <p>As an example, this is a valid global variables field:</p> <pre><code>{\n  \"outliers_filtering\": true,\n  \"data_check\": true,\n  \"pushdown\": \"memory\",\n  \"idle_timeout_in_minutes\": 30,\n  \"check_interval_for_timeout_in_seconds\": 60\n}\n</code></pre> <p>Details for each variable are below. </p>"},{"location":"variables/#user-preferences","title":"User preferences","text":""},{"location":"variables/#outlier-filtering","title":"Outlier filtering","text":"<p>If the key <code>outliers_filtering</code> is set to <code>true</code>, the app will automatically remove the top and bottom 1.5% of values. This setting is designed as an easy way to improve validity of the analysis (outliers can skew your insight as they occur infrequently but can vary wildly from the statistical center of your data). It should however be noted this comes at the cost of control. In any case the preferred way of filtering data is in your prepare step (outlier filtering applied before the data is used in Parameters Analyzer). </p> <pre><code>{\n  \"outliers_filtering\": true\n}\n</code></pre> <p>If you have outliers filtered in the dataset you are using Parameters Analyzer with, make sure to put this value to <code>false</code> or you will filter unnecessarily. </p>"},{"location":"variables/#data-checks","title":"Data checks","text":"<p>If the key <code>data_check</code> is set to <code>true</code>, the scenario will include a sanity check on your column naming and schema. The exact working is detailed under Scenario</p> <pre><code>{\n  \"data_check\": true\n}\n</code></pre>"},{"location":"variables/#technical-variables","title":"Technical variables","text":""},{"location":"variables/#processing-mode","title":"Processing Mode","text":"<p>Version 2.3 of Parameters Analyzer introduced a difference in processing mode: <code>pushdown</code> or <code>memory</code>. In pushdown mode, all computations are run using the database engine that holds the dataset. In memory mode, all data and computations are loaded into the web application memory. Performance considerations and more details are in the specs.</p> <pre><code>{\n  \"pushdown\": \"memory | pushdown\"\n}\n</code></pre>"},{"location":"variables/#idle-shutdown","title":"Idle shutdown","text":"<p>As every user runs their own instance of Parameters Analyzer, applications that are not in use will still occupy a part of the overall resources. To free up these resources, instances of Parameters Analyzer can shut themselves down when no activity is recorded after a specified timeframe and check interval. </p> <p>Generally, a lower idle_timeout will free up more unused resources, at the expense of user convenience. It is advised to set the idle_timeout to a value over 30 minutes to make sure there are no unintended shutdowns.</p> <pre><code>{\n  \"idle_timeout_in_minutes\": 30,\n  \"check_interval_for_timeout_in_seconds\": 60\n}\n</code></pre>"},{"location":"walkthrough/","title":"Walkthrough","text":""},{"location":"walkthrough/#1-install-application","title":"1. Install Application","text":"<p>Parameters Analyzer installs as a Dataiku Application. Once installed, find it on your application homepage.</p>"},{"location":"walkthrough/#2-create-new-instance","title":"2. Create New Instance","text":"<p>Click CREATE APP INSTANCE to create a new instance of the parent project. Create multiple instances to analyze different datasets.</p>"},{"location":"walkthrough/#3-application-start-screen","title":"3. Application Start Screen","text":"<p>Navigate to your instance to find the start screen, where you'll connect Parameters Analyzer to your data.</p> <p></p>"},{"location":"walkthrough/#31-project-selection","title":"3.1 Project Selection","text":"<p>Select your data preparation project from the list.</p>"},{"location":"walkthrough/#32-input-type-selection","title":"3.2 Input Type Selection","text":"<p>Select your dataset type according to your data model.</p>"},{"location":"walkthrough/#33-date-selection","title":"3.3 Date Selection","text":"<p>Your dataset(s) must include at least one parsed date. Once you've selected the project and dataset(s), available parsed dates will appear.</p>"},{"location":"walkthrough/#34-run","title":"3.4 Run","text":"<p>Click RUN after completing the previous steps.</p> <p>If you don't complete these steps, your run will fail.</p>"},{"location":"walkthrough/#35-open-parameters-analyzer","title":"3.5 Open Parameters Analyzer","text":"<p>After a successful run, click OPEN PARAMETERS ANALYZER.</p> <p>Start by launching a new analysis or explore saved studies.</p>"},{"location":"walkthrough/#launch-a-new-analysis","title":"Launch a New Analysis","text":""},{"location":"walkthrough/#target-definition","title":"Target Definition","text":"<ol> <li>Select the date range in the period section (entire range is auto-selected)</li> <li>Select a target variable (the outcome you want to analyze)</li> <li>Click Apply</li> </ol> <p>Next, select the OK range or output quality classification of your target variable. The chart displays the target variable distribution:</p> <p></p> <p>Add filters for numerical and categorical variables if needed.</p>"},{"location":"walkthrough/#variables-selection","title":"Variables Selection","text":"<p>In the second tab, select variables to analyze. Type in the field to filter.</p> <p>Once selected, adjust the number of bins (10-30, each bin appears as a circle on the area chart). Learn more about binning in methodology.</p> <p></p>"},{"location":"walkthrough/#run-analysis","title":"Run Analysis","text":"<p>Click run to get results: The most impacting variables on your selected outcome are ranked by correlation factor. Each chart shows:</p> <ul> <li>Variable name and type in the title</li> <li>Average NOK rate (percentage of NOK points calculated on non-empty rows)</li> <li>Rank and correlation of the variable</li> </ul> <p>In bubble charts:</p> <ul> <li>Bubble size shows points in each bubble</li> <li>Horizontal line shows the interval</li> <li>Dashed line shows average NOK rate</li> <li>Green color indicates NOK rate below average</li> </ul> <p>In expert mode, the same information appears as a histogram:</p> <ul> <li>Gray histogram shows number of points</li> <li>Bars start from average NOK rate line</li> <li>Upward bars show defect rates above average</li> <li>Downward bars show defect rates below average</li> </ul> <p>Comparison of display modes:</p> <p></p> <p>Standard mode chart</p> <p></p> <p></p> <p>Expert mode chart</p> <p></p>"},{"location":"walkthrough/#define-a-new-rule","title":"Define a New Rule","text":"<p>Define a rule by combining parameter intervals. Select a variable, adjust the range or select modalities, and click save range.</p> <p></p> <p>At the top, you'll see:</p> <ul> <li>Initial population (points and NOK rate in the analysis)</li> <li>Currently selected population (points and NOK rate in selected ranges)</li> <li>Ratio (percentage of initial population and improvement in selected ranges)</li> </ul> <p>Results always show the combination of all selected ranges.</p>"},{"location":"walkthrough/#explore-saved-studies-and-work-on-a-copy","title":"Explore Saved Studies and Work on a Copy","text":"<p>Go directly to the fourth tab to check existing saved studies. Select a study to display, then work on a copy to reload the study with its saved analysis parameters.</p> <p></p>"}]}